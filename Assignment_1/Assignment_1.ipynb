{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "## Objective of this assignment\n",
    "The objective is to get acquainted with the Python language, with emphasis on its scientific and numerical extensions, how data can be imported from other data sources, linear algebra basics that is needed in this course, visualization using principal component analysis (PCA) and the concept of similarity. Upon completing this exercise it is expected that you:\n",
    "\n",
    "- Understand how data can be represented as vectors and matrices in numerical Python (NumPy).\n",
    "- Can apply and interpret principal component analysis (PCA) for data visualization.\n",
    "- Understand the various measures of similarity such as Jaccard and Cosine similarity and apply similarity measures to query for similar observations.\n",
    "\n",
    "## ** Important: ** When handing in your homework:\n",
    "+ Hand in the notebook (and nothing else) named as follows: StudentName1_snumber_StudentName2_snumber.ipynb\n",
    "+ Provide clear and complete answers to the questions below under a seperate header (not hidden some-where in your source code), and make sure to explain your answers / motivate your choices. Add Markdown cells where necessary.\n",
    "+ Source code, output graphs, derivations, etc., should be included in the notebook.\n",
    "+ Hand-in: upload to Blackboard.\n",
    "+ Include name, student number, assignment (especially in filenames)!\n",
    "+ When working in pairs only one of you should upload the assignment, and report the name of your partner in your filename.\n",
    "+ For problems or questions: use the BB discussion board or email the student assistants.\n",
    "\n",
    "## Advised Reading and Exercise Material\n",
    "**The following on-line materials are recommended:**\n",
    "\n",
    "- <http://docs.python.org/tutorial> - Introduction into Python environment, syntax and data structures. Recommended reading - sections 1, 2, 3, 4 and 5.\n",
    "- <http://www.scipy.org/Tentative_NumPy_Tutorial> - Tutorial introducing the scientific computing in Python, array and matrix operations, indexing and slicing matrices.\n",
    "- <http://www.scipy.org/NumPy_for_Matlab_Users> - Useful reference to scientific computing in Python if you have previous experience with MATLAB programming.\n",
    "- <https://www.datacamp.com/courses/intro-to-python-for-data-science> - Simple introduction to Data Science using Python.\n",
    "- <http://matplotlib.sourceforge.net> - Documentation and examples related to matplotlib module, which we shall use extensively through the course to visualize data and results.\n",
    "- Pang-Ning Tan, Michael Steinbach, and Vipin Kumar, {\\em Introduction to Data Mining}, sections 2.1-2.3 + (A) + B.1\n",
    "- <http://http://www.youtube.com/course?list=ECEA1FEF17E1E5C0DA> - Series of video tutorials covering basics of Python programming.\n",
    "- Pang-Ning Tan, Michael Steinbach, and Vipin Kumar, {\\em Introduction to Data Mining}, sections 2.4 + 3.1-3.2 + C.1-C.2\n",
    "\n",
    "\n",
    "## 1.1 Python and Linear Algebra basics\n",
    "\n",
    "**For this course we advise to only use NumPy ndarrays ndarrays to represent vectors and matrices. The numpy.matrix data type, although intuitive, is less supported and uses operators for multiplication differently. This means that you can't perform matrix multiplications symbollically, but that you will have to use functions from the NumPy library!** \n",
    "\n",
    "**1.1.1** (0.3 points) Generate (and print) the following vectors using functions from the *NumPy* package in Python: \n",
    "*Note: You do not have to print column vectors as columns!*\n",
    "\n",
    "\\begin{equation}\n",
    "     \\textbf{x} = \\begin{pmatrix} \n",
    "         6 \\\\\n",
    "         7 \\\\\n",
    "         8 \\\\\n",
    "         9 \\\\\n",
    "         10 \\\\\n",
    "         11 \\\\\n",
    "         12\n",
    "       \\end{pmatrix}\n",
    "     \\textbf{y} = \\begin{pmatrix} \n",
    "         3 \\\\\n",
    "         7 \\\\\n",
    "         11 \\\\\n",
    "         15 \\\\\n",
    "         19 \\\\\n",
    "         23 \\\\\n",
    "         27\n",
    "       \\end{pmatrix}\n",
    "     \\textbf{w} = \\begin{pmatrix} \n",
    "         1 \\\\\n",
    "         1 \\\\\n",
    "         0 \\\\\n",
    "         0.5 \\\\\n",
    "         1 \\\\\n",
    "         1.5 \\\\\n",
    "         2 \\\\\n",
    "         0 \\\\\n",
    "         0 \n",
    "       \\end{pmatrix}\n",
    "     \\textbf{s} = \\begin{pmatrix}\n",
    "         100 \\\\\n",
    "         98.8 \\\\\n",
    "         97.6 \\\\\n",
    "         96.4 \\\\\n",
    "         95.2\n",
    "       \\end{pmatrix} \n",
    "     \\textbf{z} = \\begin{pmatrix}\n",
    "         0.7 \\\\\n",
    "         1.0 \\\\\n",
    "         1.3 \\\\\n",
    "         1.6 \\\\\n",
    "         1.9 \\\\\n",
    "         2.2 \\\\\n",
    "         2.5 \\\\\n",
    "         2.8\n",
    "       \\end{pmatrix}\n",
    "  \\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import NumPy\n",
    "import numpy as np\n",
    "#You'll have to manually import libraries in the future!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6  7  8  9 10 11 12]\n",
      "[ 3  4 11 15 19 23 27]\n",
      "[ 1.   1.   0.   0.5  1.   1.5  2.   0.   0. ]\n",
      "[ 100.    98.8   97.6   96.4   95.2]\n",
      "[ 0.7  1.   1.3  1.6  1.9  2.2  2.5  2.8]\n"
     ]
    }
   ],
   "source": [
    "##Generate the vectors in  using Python and NumPy \n",
    "\n",
    "x = np.array([6,7,8,9,10,11,12])\n",
    "print(x)\n",
    "y = np.array([3,4,11,15,19,23,27])\n",
    "print(y)\n",
    "w = np.array([1,1,0,0.5,1,1.5,2,0,0])\n",
    "print(w)\n",
    "s = np.array([100,98.8,97.6,96.4,95.2])\n",
    "print(s)\n",
    "z = np.array([0.7,1.0,1.3,1.6,1.9,2.2,2.5,2.8])\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then compute the following operations:\n",
    "> a. (0.2 points) **v** = 3**x** + **y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21 25 35 42 49 56 63]\n"
     ]
    }
   ],
   "source": [
    "##Answer to question 1.1.1a\n",
    "v = 3 * x + y\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> b. (0.2 points) Compute the dot product between **x** and **y** and name it **q**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036\n"
     ]
    }
   ],
   "source": [
    "##Answer to question 1.1.1b\n",
    "q = np.dot(x,y)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> c. (0.2 points) **t** = pi(**s** + 4) (element wise multiplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 326.72563597  322.95572479  319.1858136   315.41590242  311.64599124]\n"
     ]
    }
   ],
   "source": [
    "##Answer to question 1.1.1c\n",
    "t = np.pi*(s+4)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> d. (0.2 points) **z** = **z** - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3  0.   0.3  0.6  0.9  1.2  1.5  1.8]\n"
     ]
    }
   ],
   "source": [
    "##Answer to question 1.1.1d\n",
    "z = z-1\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> e. (0.2 points) replace some values of x, such that the last three values in the vector are 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 7 8 9 4 4 4]\n",
      "[6 7 8 9 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "##Answer to question 1.1.1e\n",
    "\n",
    "alternativeSolution = x \n",
    "x[x >= 10] = 4\n",
    "\n",
    "#Or based on slicing\n",
    "\n",
    "alternativeSolution[4:] = 4\n",
    "\n",
    "print(x)\n",
    "print(alternativeSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> f. (0.2 points) **r** = 2**w** - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3. -3. -5. -4. -3. -2. -1. -5. -5.]\n"
     ]
    }
   ],
   "source": [
    "##Answer to question 1.1.1f\n",
    "r = 2 * w - 5\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.2** (0.25 points) Generate these matrices:\n",
    "\n",
    "\\begin{equation}\n",
    "     \\textbf{M} = \\begin{pmatrix} \n",
    "         1 & 2 & 3 \\\\\n",
    "         6 & 8 & 4 \\\\\n",
    "         6 & 7 & 5          \n",
    "         \\end{pmatrix}\n",
    "     \\textbf{N} = \\begin{pmatrix} \n",
    "         4 & 6 \\\\\n",
    "         7 & 2 \\\\\n",
    "         5 & 1\n",
    "         \\end{pmatrix}\n",
    "     \\textbf{P} = \\begin{pmatrix} \n",
    "         2 & 5 \\\\\n",
    "         5 & 5 \n",
    "         \\end{pmatrix}   \n",
    "\\end{equation}\n",
    "\n",
    "Afterwards try and compute the operations for subquestions *a* up to and including *e* and print the resulting matrix. If some operations yield errors, give the reason as to why that happens.\n",
    "\n",
    "*A hint: NumPy has functions for matrix operations you can, and should, use!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [6 8 4]\n",
      " [6 7 5]]\n",
      "[[4 6]\n",
      " [7 2]\n",
      " [5 1]]\n",
      "[[2 5]\n",
      " [5 5]]\n"
     ]
    }
   ],
   "source": [
    "##Generate the matrices using Python and NumPy\n",
    "\n",
    "M = np.matrix([[1,2,3],[6,8,4],[6,7,5]])\n",
    "N = np.matrix([[4,6],[7,2],[5,1]])\n",
    "P = np.matrix([[2,5],[5,5]])\n",
    "print(M)\n",
    "print(N)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> a. (0.2 points) **A** = **MN** + **N**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 37  19]\n",
      " [107  58]\n",
      " [103  56]]\n"
     ]
    }
   ],
   "source": [
    "##Answer to question 1.1.2a\n",
    "A = M * N + N\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> b. (0.2 points) **B** = **N**<sup>T</sup>**M**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[76 99 65]\n",
      " [24 35 31]]\n"
     ]
    }
   ],
   "source": [
    "##Answer to question 1.1.2b\n",
    "\n",
    "##Answer to question 1.1.2b\n",
    "B = np.matmul(np.transpose(N),M)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> c. (0.2 points) **C** = **P**<sup>-1</sup> + **P**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.66666667  5.33333333]\n",
      " [ 5.33333333  4.86666667]]\n"
     ]
    }
   ],
   "source": [
    "##Answer to question 1.1.2c\n",
    "C = np.linalg.inv(P) + P\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> d. (0.2 points) **D** = **AC**(**C** + **B**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,2) (2,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-266-ba8772934d43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##Answer to question 1.1.2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#will yield an error because C+B is an impossible action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,2) (2,3) "
     ]
    }
   ],
   "source": [
    "##Answer to question 1.1.2d\n",
    "#will yield an error because C+B is an impossible action\n",
    "D = np.matmul(A,np.matmul(C, C+B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> e. (0.2 points) Compute the eigenvalues and eigenvectors of **M**, **N**, and **P** (and print them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added a column of zeroes to counter dimensionality issues calculating the eigenvalues.\n",
    "print(\"eigenvalues M: \")\n",
    "print(np.linalg.eig(M))\n",
    "print(\"eigenvalues N: \")\n",
    "zeroesN = np.zeros((3,1),dtype=np.int64)\n",
    "print(np.linalg.eigvals(np.append(N, zeroesN, axis=1)))\n",
    "print(\"eigenvalues P: \")\n",
    "print(np.linalg.eigvals(P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Principal Component Analysis\n",
    "\n",
    "**1.2.1** many experimenters have a habit of using Microsoft Excel as their tool to record measurements from experiments. Fortunately Python can read Excel files. Various methods exist, of which the use of the Python library Pandas is arguably one of the easiest for the purpose of Data Mining.\n",
    "\n",
    "The data used in this exercise is based on data from a chemical sensor obtained from the NanoNose project[1]. The data contains 8 sensors, named by the letters A-H, measuring the concentration of Water, Ethanol, Acetone, Heptane and Pentanol injected into a small gas chamber. The data will be represented in matrix form such that each column contains the 8 sensor measurements (i.e., sensor A-H) of the various compounds injected into the gas chamber.\n",
    "> a. (0.2 points) Inspect the nanonose.xls file in the Data folder and make sure you understand how the data is stored in Excel.\n",
    "\n",
    "> *Load the data in python using the Pandas library (use the read_excel() function). Make sure you read some of the Pandas documentation! Especially slicing, indexing and dropping are useful commands*\n",
    "\n",
    "> *make sure to inspect the data first! You should cut out some rows and columns! You can easily inspect the structure of the Pandas dataframe using the head() function.*\n",
    "\n",
    "> *Finally use the as_matrix() function to cast the Pandas dataframe to a NumPy array. Note: counterintuitively, this yields an object of type: numpy.ndarray, not of type numpy.matrix!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data1 = pd.read_excel('Data/nanonose.xls')\n",
    "\n",
    "\n",
    "#Removed 'empty' row, kept important headers\n",
    "data2 = data1.iloc[1:]\n",
    "#Removed empty column\n",
    "data2 = data2.drop(data2.columns[2], axis=1)\n",
    "#Inspect the data manually\n",
    "\n",
    "print(data2.head())\n",
    "\n",
    "#Creating dataset for SVD for future use\n",
    "dataSVD = data2.iloc[:, 2:9]\n",
    "#Inspect SVD Dataset\n",
    "print(dataSVD.head())\n",
    "#Casting the pandas dataframe to a NumPy matrix\n",
    "dataAsNumpy = data2.as_matrix()\n",
    "\n",
    "dataAsNumpy.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> b. (0.3 points) The data resides in what can be seen as an 8-dimensional space. Each dimension (column), corresponds to one of the 8 NanoNose sensors. Multidimensional (>3) data is hard to visualize, as we are unable to plot that many dimensions simultaneously.\n",
    "\n",
    "> Using the matplotlib library, plot the attributes A and B against each other. After you have plotted A against B, also plot a few other combinations of attributes. Plot at least 4 different combinations. \n",
    "\n",
    "> *Do not forget to label your axes and add a figure description!*\n",
    "\n",
    "> NOTE: If you want to plot inside the Jupyter notebook use the following command after you imported the library: \n",
    "*%matplotlib inline* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#Seperating columns for easy plotting\n",
    "A = data2['A']\n",
    "B = data2['B']\n",
    "C = data2['C']\n",
    "D = data2['D']\n",
    "E = data2['E']\n",
    "F = data2['F']\n",
    "G = data2['G']\n",
    "H = data2['H']\n",
    "\n",
    "#Plotting the seperate columns\n",
    "plt.figure(0)\n",
    "plt.scatter(A, B)\n",
    "plt.xlabel('Sensor A \\n Scatterplot sensor A~B', fontsize=14)\n",
    "plt.ylabel('Sensor B', fontsize=14)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.scatter(F, G)\n",
    "plt.xlabel('Sensor F \\n Scatterplot sensor F~G', fontsize=14)\n",
    "plt.ylabel('Sensor G', fontsize=14)\n",
    "\n",
    "\n",
    "plt.figure(2)\n",
    "plt.scatter(D, E)\n",
    "plt.xlabel('Sensor D \\n Scatterplot sensor D~E', fontsize=14)\n",
    "plt.ylabel('Sensor E', fontsize=14)\n",
    "\n",
    "\n",
    "plt.figure(3)\n",
    "plt.scatter(F, A)\n",
    "plt.xlabel('Sensor F \\n Scatterplot sensor F~A', fontsize=14)\n",
    "plt.ylabel('Sensor A', fontsize=14)\n",
    "\n",
    "\n",
    "plt.figure(4)\n",
    "plt.scatter(C, H)\n",
    "plt.xlabel('Sensor C \\n Scatterplot sensor C~H', fontsize=14)\n",
    "plt.ylabel('Sensor H', fontsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1.2.2 Principal Component Analysis, more commonly known as PCA, can be cleverly used to better visualize high dimensional data. \n",
    "\n",
    "> a. (1 point) Explain what PCA is and when it can be used. Make sure to provide an in-depth explanation and note what the drawbacks and limitations are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Double click to type your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply PCA we must first mean center the data. mean centering means that the mean value for an attribute is subtracted from that attribute. \n",
    "\n",
    "> b. (0.5 points) Why do we first need to mean center the data before applying PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Double click to type your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, applying PCA comes down to a series of matrix operations, this is useful, as matrix operations can be applied with little effort and can be computed in relatively little time.\n",
    "\n",
    "The given data **X**, must first be mean centered. This can be easily done by calculating a row vector **μ** containing the mean values of each attribute and then subtracting it from **X** (**Y** = **X** - **1μ**, where **1** is a column vector of size **N** x 1, with N indicating the number of observations in the original data).\n",
    "\n",
    "Then, the Singular Value Decomposition, or SVD, of **Y**, the mean centered data, can be calculated. **Y** = **USV**<sup>T</sup>. In practice, this is often done using the numpy.linalg.svd() function.\n",
    "\n",
    "Using SVD on **Y** yields a series of vectors which can be used to project the data onto specific Principal Components (PCs). The entire dataset can be projected onto the Principal Components by multiplying **Z** = **Y\\*V**, where **Z** indicates the projected data. Alternatively, one could project onto just a subset of all the PCs by indexing in the multiplication. For example: **Z** = **Y\\*V[:,1]** would yield the projection of the data onto the first PC.\n",
    "\n",
    "> c. (1 point)  Apply PCA (using the aforementioned method) on the Nanonose data and visualize a scatterplot of the projection of the data onto the first two PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Mean centering every column by substracting every element\n",
    "#by the mean of the column\n",
    "AMeanCentered = A -A.mean()\n",
    "BMeanCentered = B -B.mean()\n",
    "CMeanCentered = C -C.mean()\n",
    "DMeanCentered = D -D.mean()\n",
    "EMeanCentered = E -E.mean()\n",
    "FMeanCentered = F -F.mean()\n",
    "GMeanCentered = G -G.mean()\n",
    "HMeanCentered = H -H.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "meanCenteredDf = pd.concat([AMeanCentered, BMeanCentered, CMeanCentered, DMeanCentered, \n",
    "            EMeanCentered, FMeanCentered, GMeanCentered, HMeanCentered], axis=1)\n",
    "Y = meanCenteredDf.as_matrix()\n",
    "\n",
    "U, S,V = np.linalg.svd(Y)\n",
    "\n",
    "\n",
    "\n",
    "Z = np.matmul(Y,V[:,0:2])\n",
    "\n",
    "\n",
    "plt.figure(5)\n",
    "plt.scatter(Z[:,[0]],Z[:,[1]])\n",
    "plt.xlabel('Principle component 1 \\n PCA first 2 PCs', fontsize=14)\n",
    "plt.ylabel('Principle component', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> d. (1 point) Alternatively, one could use EigenValue Decomposition, EVD, instead of SVD. What are the similarities and differences between SVD and EVD? Can both be applied in each case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Double click to type your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA attempts to explain as much of the variance in data with as few PCs as possible. The variance explained by each of the PCs **m** can be calculated using the following formula: \n",
    "$\\rho_m = 1 - \\frac{\\|{Y} - {u}_m s_{mm} {v}_m^T\\|^2_F}{\\|{Y}\\|^2_F} = \\frac{s_{mm}^2}{\\displaystyle \\sum_{m'=1}^M s^2_{m'm'}}$\n",
    "\n",
    "Which indicates that the variation $\\rho$ for a given **m** can be calculated by dividing the squared singular value of component **m** by the sum of all squared singular values. \n",
    "> e. (1 point) Calculate the row vector $\\rho$ containing all values of $\\rho$ for all PCs **m**. Create a bar plot with the variance explained on the Y axis and the number of the PC on the X axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sumS = sum(s)\n",
    "m = (S**2)/  sumS\n",
    "print(m)\n",
    "n_groups = 8\n",
    "index = np.arange(n_groups)\n",
    "plt.figure(6)\n",
    "plt.bar(index, m)\n",
    "plt.xlabel('All 8 Principal components \\n variance explained by PC' , fontsize=14)\n",
    "plt.ylabel('Variance', fontsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> f. (0.5 points) How much of the variance is explained by the first three PCs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the variance explained by the first three PCs is calculated by\n",
    "# adding them up\n",
    "varianceFirstThree = m[0] + m[1] + m[2]\n",
    "print(varianceFirstThree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Double click to type your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of **V** indicate the exact projection of the data onto the PCs. In a way, a PC is nothing other than a linear combination of the original attributes. \n",
    "> g. (0.5 points) Which attributes are primarily represented by the first PC? What would cause an observation to have a large negative/positive projection onto the second principal component?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Double click to type your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Similarity measures\n",
    "\n",
    "We will use a subset of the data on wild faces described by Berg in 2005 transformed to a total\n",
    "of 1000 gray scale images of size 40x40 pixels, we will attempt to find faces in the\n",
    "data base that are the most similar to a given query face. To measure similarity we\n",
    "will consider the following measures: SMC, Jaccard, Cosine, ExtendedJaccard, and\n",
    "Correlation. These measures of similarity are described in *Introduction to Data Mining*, page 73-77 and are given by\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\textrm{SMC}({x},{y}) & = & \\frac{\\textrm{Number of matching attribute values}}{\\textrm{Number of attributes}} \\\\\n",
    "\\textrm{Jaccard}({x},{y}) & = & \\frac{\\textrm{Number of matching presences}}{\\textrm{Number of attributes not involved in 00 matches}} \\\\\n",
    "\\textrm{Cosine}({x},{y}) & = & \\frac{{x}^T {y}}{\\|{x}\\| \\|{y}\\|} \\\\\n",
    "\\textrm{ExtendedJaccard}({x},{y}) & = & \\frac{{x}^T {y}}{\\|{x}\\|^2 + \\|{y}\\|^2 - {x}^T {y}} \\\\\n",
    "\\textrm{Correlation}({x},{y}) & = & \\frac{\\textrm{cov}({x},{y})}{\\textrm{std}({x}) \\textrm{std}({y})}\n",
    "\\end{eqnarray*}\n",
    "where $\\textrm{cov}({x},{y})$ denotes the covariance between ${x}$ and ${y}$ and $\\textrm{std}({x})$ denotes the standard deviation of ${x}$.\n",
    "\n",
    "Notice that the SMC and Jaccard similarity measures only are defined for binary\n",
    "data, i.e., data that takes values in $\\{0,1\\}$. As the data we analyze is non-binary,\n",
    "we will transform the data to be binary when calculating these two measures of\n",
    "similarity by setting\n",
    "$x_i = \\left\\{ \\begin{array}{ll} 0 & \\textrm{if~} x_i < \\textrm{median}({x}) \\\\\n",
    "                                1 & \\textrm{otherwise.} \\end{array} \\right.$\n",
    "                                \n",
    "### 1.3.1\n",
    "> (0.5 points) Inspect and run the simfaces function from the Toolbox. The function loads the CBCL face database, computes the similarity between a selected query image and all others, and display the query image, the 5 most similar images, and the 5 least similar images. The value of the used similarity measure is shown below each image. Try changing the query image and the similarity measure and see what happens. Which similarity measures produce similar results? Which one gives the best result? Why?\n",
    "\n",
    "> Give a quick overview of the settings (image number and similarity measure) for each time you run the script! Remember to leave all results open!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Toolbox.simfaces import simfaces\n",
    "\n",
    "simfaces(587,'cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar results; jaccard, smc (yield the same faces with slightly different similarity scores) best result; cosine seems to be yielding the best results when judging from a visual aspect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> (0.75 points) We will investigate how scaling and translation impact the following three\n",
    "similarity measures: Cosine, ExtendedJaccard, and Correlation. Let $\\alpha$ and $\\beta$ be two constants. Using Python, calculate the following similarity measures, and check if the statements below are correct.\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\textrm{Cosine}(\\alpha{x},{y}) & = & \\textrm{Cosine}({x},{y}) \\\\\n",
    "\\textrm{ExtendedJaccard}(\\alpha{x},{y}) & = & \\textrm{ExtendedJaccard}({x},{y}) \\\\\n",
    "\\textrm{Correlation}(\\alpha{x},{y}) & = & \\textrm{Correlation}({x},{y}) \\\\\n",
    "\\textrm{Cosine}(\\beta + {x},{y}) & = & \\textrm{Cosine}({x},{y}) \\\\\n",
    "\\textrm{ExtendedJaccard}(\\beta + {x},{y}) & = & \\textrm{ExtendedJaccard}({x},{y}) \\\\\n",
    "\\textrm{Correlation}(\\beta + {x},{y}) & = & \\textrm{Correlation}({x},{y})\n",
    "\\end{eqnarray*}\n",
    "\n",
    "> Type help similarity, or study similarity.py, to learn about the function that is used to compute the similarity measures. Do not forget to also import similarity.py!\n",
    "\n",
    "> Even though a similarity measure is theoretically invariant e.g.\\ to scaling, it might not be exactly invariant numerically.\n",
    "\n",
    "> Do not forget to also provide an explanation in addition to the calculations!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine with constant multiplication: 2 matrices are the same so statement is correct\n",
      "[[ 0.86770053  0.94499463  0.99463045  0.68461117]\n",
      " [ 0.81262809  0.93092623  0.99378637  0.5889941 ]\n",
      " [ 0.9553625   0.93661781  0.87392146  0.79483254]\n",
      " [ 0.93588892  0.94061523  0.79186077  0.60673901]]\n",
      "[[ 0.86770053  0.94499463  0.99463045  0.68461117]\n",
      " [ 0.81262809  0.93092623  0.99378637  0.5889941 ]\n",
      " [ 0.9553625   0.93661781  0.87392146  0.79483254]\n",
      " [ 0.93588892  0.94061523  0.79186077  0.60673901]]\n",
      "extended jaccard with constant multiplication: 2 matrices are the same so statement is correct\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "correlation with constant multiplication: 2 matrices are the same so statement is correct\n",
      "[[ 0.30448583  0.49168072  0.98914949 -0.1445224 ]\n",
      " [ 0.13646793  0.53896883  0.96389778 -0.3328999 ]\n",
      " [ 0.9114815   0.38672261 -0.13599852  0.42139921]\n",
      " [ 0.76642656  0.79423437  0.05204032 -0.06722775]]\n",
      "[[ 0.30448583  0.49168072  0.98914949 -0.1445224 ]\n",
      " [ 0.13646793  0.53896883  0.96389778 -0.3328999 ]\n",
      " [ 0.9114815   0.38672261 -0.13599852  0.42139921]\n",
      " [ 0.76642656  0.79423437  0.05204032 -0.06722775]]\n",
      "cosine with constant addition: 2 matrices are not the same so statement is incorrect\n",
      "[[ 0.86770053  0.94499463  0.99463045  0.68461117]\n",
      " [ 0.81262809  0.93092623  0.99378637  0.5889941 ]\n",
      " [ 0.9553625   0.93661781  0.87392146  0.79483254]\n",
      " [ 0.93588892  0.94061523  0.79186077  0.60673901]]\n",
      "[[ 0.87375372  0.9508336   0.9703709   0.7236755 ]\n",
      " [ 0.84810726  0.95251835  0.98869146  0.66143616]\n",
      " [ 0.93353226  0.94421248  0.89732531  0.78632818]\n",
      " [ 0.93426943  0.97652555  0.89478157  0.70571011]]\n",
      "extended jaccard with constant addition: 2 matrices are the same so statement is correct\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "correlation with constant addition: 2 matrices are the same so statement is correct\n",
      "[[ 0.30448583  0.49168072  0.98914949 -0.1445224 ]\n",
      " [ 0.13646793  0.53896883  0.96389778 -0.3328999 ]\n",
      " [ 0.9114815   0.38672261 -0.13599852  0.42139921]\n",
      " [ 0.76642656  0.79423437  0.05204032 -0.06722775]]\n",
      "[[ 0.30448583  0.49168072  0.98914949 -0.1445224 ]\n",
      " [ 0.13646793  0.53896883  0.96389778 -0.3328999 ]\n",
      " [ 0.9114815   0.38672261 -0.13599852  0.42139921]\n",
      " [ 0.76642656  0.79423437  0.05204032 -0.06722775]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdm/RU/DataMining/Assignment_1/Toolbox/similarity.py:44: RuntimeWarning: overflow encountered in exp\n",
      "  sim = XYt / (np.log( np.exp(sum(np.power(X.T,2))).T * np.exp(sum(np.power(Y.T,2))) ) - XYt)\n"
     ]
    }
   ],
   "source": [
    "from Toolbox.similarity import similarity\n",
    "import numpy as np\n",
    "matrixone = np.random.random((4,4))\n",
    "matrixtwo = np.random.random((4,4))\n",
    "matrixone = (matrixone*100).astype(int)\n",
    "matrixtwo = (matrixtwo*100).astype(int)\n",
    "#print(matrixone)\n",
    "#print(matrixtwo)\n",
    "#defining constants:\n",
    "A = 20\n",
    "B = 35\n",
    "\n",
    "#Cosine calculations:\n",
    "print(\"cosine with constant multiplication: 2 matrices are the same so statement is correct\")\n",
    "#vanilla:\n",
    "print(similarity(matrixone,matrixtwo,'cos'))\n",
    "#multiplied by constant:\n",
    "print(similarity(matrixone*A,matrixtwo,'cos'))\n",
    "\n",
    "#extendedjaccard:\n",
    "print(\"extended jaccard with constant multiplication: 2 matrices are the same so statement is correct\")\n",
    "#vanilla:\n",
    "print(similarity(matrixone,matrixtwo,'ext'))\n",
    "#multiplied by constant:\n",
    "print(similarity(matrixone*A,matrixtwo,'ext'))\n",
    "\n",
    "#Correlation:\n",
    "print(\"correlation with constant multiplication: 2 matrices are the same so statement is correct\")\n",
    "#vanilla:\n",
    "print(similarity(matrixone,matrixtwo,'cor'))\n",
    "#multiplied by constant:\n",
    "print(similarity(matrixone*A,matrixtwo,'cor'))\n",
    "\n",
    "#with constant addition:\n",
    "#cosine calculations:\n",
    "print(\"cosine with constant addition: 2 matrices are not the same so statement is incorrect\")\n",
    "#vanilla:\n",
    "print(similarity(matrixone,matrixtwo,'cos'))\n",
    "#with constant addition\n",
    "print(similarity(matrixone+B,matrixtwo,'cos'))\n",
    "\n",
    "#extended jaccard calculations:\n",
    "print(\"extended jaccard with constant addition: 2 matrices are the same so statement is correct\")\n",
    "#vanilla:\n",
    "print(similarity(matrixone,matrixtwo,'ext'))\n",
    "#with constant addition\n",
    "print(similarity(matrixone+B,matrixtwo,'ext'))\n",
    "\n",
    "#correlation calculations:\n",
    "print(\"correlation with constant addition: 2 matrices are the same so statement is correct\")\n",
    "#vanilla:\n",
    "print(similarity(matrixone,matrixtwo,'cor'))\n",
    "#with constant addition\n",
    "print(similarity(matrixone+B,matrixtwo,'cor'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using randomly generated matrices of the same size, and the similarity calculations; the results can be compared to see whether they differ from each other. for the constant of a the number 20 was chosen, for constant b the number 35. all other calculations were performed using numpy and the given library. furthermore, the print statements show what kind of calculation was executed along with the method, the exact code was commented so you can tell which matrix in the printed output belongs to which calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning in the Jupyter Notebook\n",
    "Do not forget to read the provided guidelines regarding the expectations and grading of your report. You can find these on Blackboard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Tommy S Alstrøm, Jan Larsen, Claus H Nielsen, and Niels B Larsen. Data-driven\n",
    "modeling of nano-nose gas sensor arrays. In SPIE Defense, Security, and Sensing,\n",
    "pages 76970U\u001576970U. International Society for Optics and Photonics, 2010. URL\n",
    "http://www.nanonose.dk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
